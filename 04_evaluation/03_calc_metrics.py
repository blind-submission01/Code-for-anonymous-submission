"""
è¯„æµ‹æŒ‡æ ‡è®¡ç®—è„šæœ¬ï¼ˆé«˜é€Ÿç‰ˆ v2ï¼‰ï¼š
- BERTScore æ‰¹é‡è®¡ç®—ï¼ˆ10-20x åŠ é€Ÿï¼‰
- ESS/RAS å¤šè¿›ç¨‹å¹¶è¡Œ
- å¤šæ¨¡å‹æ–‡ä»¶å¹¶å‘å¤„ç†
- æ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""
import json
import sys
import os
from typing import Any, Dict, List, Tuple
from statistics import mean, variance, stdev
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import multiprocessing
import threading

# ========== æ–‡ä»¶è·¯å¾„é…ç½® ==========
METRICS_DIR = ""
sys.path.insert(0, METRICS_DIR)
sys.path.insert(0, os.path.join(METRICS_DIR, "BERTScore"))
sys.path.insert(0, os.path.join(METRICS_DIR, "BLEU"))
sys.path.insert(0, os.path.join(METRICS_DIR, "ESS_and_RAS"))

from bleu_score import compute_bleu
from esra_score import compute_ess, compute_ras


REFERENCE_FILE = ""


# æ¨¡å‹è¾“å‡ºæ–‡ä»¶
MODEL_FILES = {
    "only_orpo_8B": "",
    "two_stage_orpo_14B": "",
    "two_stage_h_orpo_14B": "",

}

# è¾“å‡ºæ–‡ä»¶
OUTPUT_JSONL = ""
OUTPUT_REPORT = ""
CHECKPOINT_FILE = ""

# ========== å¹¶è¡Œé…ç½® ==========
NUM_TOTAL_CORES = multiprocessing.cpu_count()
NUM_MODEL_WORKERS = min(len(MODEL_FILES), 3)  # åŒæ—¶å¤„ç†çš„æ¨¡å‹æ–‡ä»¶æ•°ï¼ˆæ ¹æ®å†…å­˜è°ƒæ•´ï¼‰
NUM_ESS_RAS_WORKERS_PER_MODEL = max(1, (NUM_TOTAL_CORES - 2) // NUM_MODEL_WORKERS)  # æ¯ä¸ªæ¨¡å‹çš„ ESS/RAS è¿›ç¨‹æ•°

# çº¿ç¨‹é”ï¼ˆç”¨äºæ‰“å°å’Œæ£€æŸ¥ç‚¹ä¿å­˜ï¼‰
print_lock = threading.Lock()
checkpoint_lock = threading.Lock()
bertscore_lock = threading.Lock()  # æ–°å¢ï¼šBERTScore é”ï¼ˆé¿å…å¤šçº¿ç¨‹å¹¶å‘åŠ è½½æ¨¡å‹ï¼‰

def read_jsonl(path: str) -> List[Dict[str, Any]]:
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line.strip()))
    return data


def write_jsonl(path: str, items: List[Dict[str, Any]]) -> None:
    with open(path, "w", encoding="utf-8") as f:
        for item in items:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")


def safe_print(*args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°"""
    with print_lock:
        print(*args, **kwargs)

def batch_bertscore(candidates: List[str], references: List[str]) -> List[Tuple[float, float, float]]:
    """
    æ‰¹é‡è®¡ç®— BERTScoreï¼ˆæ ¸å¿ƒåŠ é€Ÿï¼‰
    æ³¨æ„ï¼šéœ€è¦åŠ é”ï¼Œé¿å…å¤šçº¿ç¨‹å¹¶å‘åŠ è½½æ¨¡å‹å¯¼è‡´å†²çª
    """
    from bert_score import score
    
    # å¤„ç†ç©ºå­—ç¬¦ä¸²
    valid_indices = []
    valid_cands = []
    valid_refs = []
    
    for i, (c, r) in enumerate(zip(candidates, references)):
        if c and r:
            valid_indices.append(i)
            valid_cands.append(c)
            valid_refs.append(r)
    
    # åˆå§‹åŒ–ç»“æœ
    results = [(0.0, 0.0, 0.0)] * len(candidates)
    
    if valid_cands:
        # ä½¿ç”¨é”ä¿æŠ¤ BERTScore è®¡ç®—ï¼ˆé¿å…å¤šçº¿ç¨‹å¹¶å‘åŠ è½½æ¨¡å‹ï¼‰
        with bertscore_lock:
            P, R, F1 = score(
                valid_cands, 
                valid_refs, 
                model_type="bert-base-uncased",
                verbose=False,
                batch_size=32
            )
        
        for idx, (p, r, f1) in zip(valid_indices, zip(P.tolist(), R.tolist(), F1.tolist())):
            results[idx] = (p, r, f1)
    
    return results


def compute_bleu_batch(pairs: List[Tuple[str, str]]) -> List[float]:
    """æ‰¹é‡è®¡ç®— BLEU"""
    results = []
    for pred, ref in pairs:
        if pred and ref:
            results.append(compute_bleu(pred, ref))
        else:
            results.append(0.0)
    return results


# å…¨å±€ spaCy æ¨¡å‹ï¼ˆç”¨äºå¤šè¿›ç¨‹åˆå§‹åŒ–ï¼‰
_nlp = None

def init_worker():
    """è¿›ç¨‹æ± åˆå§‹åŒ–ï¼šåŠ è½½ spaCy æ¨¡å‹"""
    global _nlp
    import spacy
    _nlp = spacy.load("en_core_web_sm")


def compute_ess_ras_worker(args):
    """Worker å‡½æ•°ï¼šä½¿ç”¨å…¨å±€ nlp"""
    global _nlp
    idx, pred_rc, ref_rc, pred_rs, ref_rs = args
    
    ess = compute_ess(ref_rc, pred_rc, nlp=_nlp) if pred_rc and ref_rc else 0.0
    ras = compute_ras(ref_rs, pred_rs, nlp=_nlp) if pred_rs and ref_rs else 0.0
    
    return idx, ess, ras


def compute_ess_ras_parallel(data_list: List[Tuple], num_workers: int, model_name: str = "") -> Dict[int, Tuple[float, float]]:
    """
    å¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®— ESS/RAS
    """
    results = {}
    
    with ProcessPoolExecutor(max_workers=num_workers, initializer=init_worker) as executor:
        futures = {executor.submit(compute_ess_ras_worker, item): item[0] for item in data_list}
        
        # ä½¿ç”¨ position å‚æ•°é¿å…è¿›åº¦æ¡é‡å 
        for future in tqdm(as_completed(futures), total=len(futures), 
                          desc=f"[{model_name}] ESS/RAS", ncols=80, leave=False):
            idx, ess, ras = future.result()
            results[idx] = (ess, ras)
    
    return results

def process_model_standalone(model_name: str, model_file: str, ref_dict: Dict, 
                             num_ess_workers: int) -> Tuple[str, List[Dict]]:
    """
    å¤„ç†å•ä¸ªæ¨¡å‹çš„æ‰€æœ‰æ•°æ®ï¼ˆç‹¬ç«‹å‡½æ•°ï¼Œç”¨äºå¹¶å‘è°ƒç”¨ï¼‰
    è¿”å› (model_name, results)
    """
    safe_print(f"\nğŸš€ [{model_name}] å¼€å§‹å¤„ç†...")
    safe_print(f"   [{model_name}] æ–‡ä»¶: {model_file}")
    safe_print(f"   [{model_name}] ESS/RAS è¿›ç¨‹æ•°: {num_ess_workers}")
    
    model_data = read_jsonl(model_file)
    safe_print(f"   [{model_name}] æ¨¡å‹è¾“å‡ºæ¡æ•°: {len(model_data)}")
    
    # å‡†å¤‡æ•°æ®
    valid_items = []
    for item in model_data:
        instance_id = item["instance_id"]
        ref_item = ref_dict.get(instance_id)
        if ref_item:
            valid_items.append({
                "instance_id": instance_id,
                "pred_rc": item.get("root_cause", ""),
                "pred_rs": item.get("repair_suggestion", ""),
                "ref_rc": ref_item.get("root_cause", ""),
                "ref_rs": ref_item.get("repair_suggestion", ""),
                "status": item.get("status", ""),
            })
    
    safe_print(f"   [{model_name}] æœ‰æ•ˆæ•°æ®æ¡æ•°: {len(valid_items)}")
    
    # ========== é˜¶æ®µ1ï¼šæ‰¹é‡è®¡ç®— BERTScore ==========
    safe_print(f"   [{model_name}] ğŸ“Š é˜¶æ®µ1/3: BERTScore...")
    
    rc_cands = [item["pred_rc"] for item in valid_items]
    rc_refs = [item["ref_rc"] for item in valid_items]
    rs_cands = [item["pred_rs"] for item in valid_items]
    rs_refs = [item["ref_rs"] for item in valid_items]
    
    rc_bert_results = batch_bertscore(rc_cands, rc_refs)
    rs_bert_results = batch_bertscore(rs_cands, rs_refs)

    # ========== é˜¶æ®µ2ï¼šè®¡ç®— BLEU ==========
    safe_print(f"   [{model_name}] ğŸ“Š é˜¶æ®µ2/3: BLEU...")
    rc_bleu_results = []
    rs_bleu_results = []
    
    for item in valid_items:
        rc_bleu_results.append(compute_bleu(item["pred_rc"], item["ref_rc"]) if item["pred_rc"] and item["ref_rc"] else 0.0)
        rs_bleu_results.append(compute_bleu(item["pred_rs"], item["ref_rs"]) if item["pred_rs"] and item["ref_rs"] else 0.0)
    
    # ========== é˜¶æ®µ3ï¼šå¹¶è¡Œè®¡ç®— ESS/RAS ==========
    safe_print(f"   [{model_name}] ğŸ“Š é˜¶æ®µ3/3: ESS/RAS...")
    
    ess_ras_inputs = [
        (i, item["pred_rc"], item["ref_rc"], item["pred_rs"], item["ref_rs"])
        for i, item in enumerate(valid_items)
    ]
    
    ess_ras_results = compute_ess_ras_parallel(ess_ras_inputs, num_ess_workers, model_name)
    
    # ========== æ±‡æ€»ç»“æœ ==========
    model_results = []
    
    for i, item in enumerate(valid_items):
        rc_p, rc_r, rc_f1 = rc_bert_results[i]
        rs_p, rs_r, rs_f1 = rs_bert_results[i]
        ess, ras = ess_ras_results[i]
        
        output_item = {
            "instance_id": item["instance_id"],
            "model": model_name,
            "status": item["status"],
            "rc_bertscore_p": rc_p,
            "rc_bertscore_r": rc_r,
            "rc_bertscore_f1": rc_f1,
            "rc_bleu4": rc_bleu_results[i],
            "rc_ess": ess,
            "rs_bertscore_p": rs_p,
            "rs_bertscore_r": rs_r,
            "rs_bertscore_f1": rs_f1,
            "rs_bleu4": rs_bleu_results[i],
            "rs_ras": ras,
            "esra": (ess + ras) / 2.0,
            "metrics": {
                "rc_bertscore_p": rc_p,
                "rc_bertscore_r": rc_r,
                "rc_bertscore_f1": rc_f1,
                "rc_bleu4": rc_bleu_results[i],
                "rc_ess": ess,
                "rs_bertscore_p": rs_p,
                "rs_bertscore_r": rs_r,
                "rs_bertscore_f1": rs_f1,
                "rs_bleu4": rs_bleu_results[i],
                "rs_ras": ras,
                "esra": (ess + ras) / 2.0,
            }
        }
        model_results.append(output_item)

    safe_print(f"   âœ… [{model_name}] å®Œæˆï¼Œå…± {len(model_results)} æ¡")
    return model_name, model_results

def save_checkpoint(completed_models: List[str], all_output_items: List[Dict]):
    """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with checkpoint_lock:
        checkpoint = {
            "completed_models": completed_models,
            "items_count": len(all_output_items)
        }
        with open(CHECKPOINT_FILE, "w") as f:
            json.dump(checkpoint, f)
        write_jsonl(OUTPUT_JSONL, all_output_items)


def load_checkpoint() -> Tuple[List[str], List[Dict]]:
    """åŠ è½½æ£€æŸ¥ç‚¹ï¼›è‹¥æ— æ£€æŸ¥ç‚¹ï¼Œåˆ™ä»å·²æœ‰è¾“å‡ºæ¨æ–­å·²å®Œæˆæ¨¡å‹"""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r") as f:
            checkpoint = json.load(f)
        # å…¼å®¹æ—§ç‰ˆæœ¬çš„ checkpointï¼ˆä½¿ç”¨ modeï¼‰
        completed = checkpoint.get("completed_models", checkpoint.get("completed_modes", []))
        items = read_jsonl(OUTPUT_JSONL) if os.path.exists(OUTPUT_JSONL) else []
        return completed, items

    # æ— æ£€æŸ¥ç‚¹æ—¶ï¼Œç›´æ¥ä»å·²ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶ä¸­æ¢å¤
    items = read_jsonl(OUTPUT_JSONL) if os.path.exists(OUTPUT_JSONL) else []
    completed = []
    for item in items:
        model = item.get("model", item.get("mode"))
        if model and model not in completed:
            completed.append(model)
    return completed, items

def compute_statistics(values: List[float]) -> Dict[str, float]:
    if not values:
        return {"max": 0.0, "min": 0.0, "mean": 0.0, "variance": 0.0, "std": 0.0}
    return {
        "max": max(values),
        "min": min(values),
        "mean": mean(values),
        "variance": variance(values) if len(values) > 1 else 0.0,
        "std": stdev(values) if len(values) > 1 else 0.0,
    }

def generate_markdown_report(all_results: Dict[str, List[Dict]], output_path: str) -> None:
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    lines = []
    
    lines.append("# ğŸ“Š æ¨¡å‹è¯„æµ‹æŒ‡æ ‡åˆ†ææŠ¥å‘Š")
    lines.append("")
    lines.append("> æœ¬æŠ¥å‘Šå¯¹æ¯”äº†å¤šç§æ¨¡å‹ç”Ÿæˆçš„ Root Cause å’Œ Repair Suggestion çš„è´¨é‡æŒ‡æ ‡ã€‚")
    lines.append("")
    
    lines.append("## ğŸ“‹ æŒ‡æ ‡è¯´æ˜")
    lines.append("")
    lines.append("| æŒ‡æ ‡ | è¯´æ˜ | èŒƒå›´ |")
    lines.append("|------|------|------|")
    lines.append("| **BERTScore F1** | åŸºäº BERT åµŒå…¥çš„è¯­ä¹‰ç›¸ä¼¼åº¦ | 0~1 |")
    lines.append("| **BLEU-4** | 4-gram è¯é¢åŒ¹é…åº¦ | 0~100 |")
    lines.append("| **ESS** | Error State Scoreï¼Œç¼ºé™·æ¦‚å¿µè¦†ç›–åº¦ | 0~1 |")
    lines.append("| **RAS** | Repair Action Scoreï¼Œä¿®å¤åŠ¨ä½œè¦†ç›–åº¦ | 0~1 |")
    lines.append("| **ESRA** | (ESS + RAS) / 2 ç»¼åˆå¾—åˆ† | 0~1 |")
    lines.append("")
    
    metric_keys = [
        ("rc_bertscore_f1", "RC BERTScore F1"),
        ("rc_bleu4", "RC BLEU-4"),
        ("rc_ess", "RC ESS"),
        ("rs_bertscore_f1", "RS BERTScore F1"),
        ("rs_bleu4", "RS BLEU-4"),
        ("rs_ras", "RS RAS"),
        ("esra", "ESRA"),
    ]
    
    lines.append("---")
    lines.append("")
    lines.append("## ğŸ“ˆ å„æ¨¡å‹è¯¦ç»†åˆ†æ")
    lines.append("")
    
    for model_name, results in all_results.items():
        resolved = [r for r in results if r.get("status") == "resolved"]
        failed = [r for r in results if r.get("status") == "failed"]
        
        lines.append(f"### ğŸ”¹ æ¨¡å‹: `{model_name.upper()}`")
        lines.append("")
        lines.append(f"- **æ ·æœ¬æ€»æ•°**: {len(results)}")
        lines.append(f"- **Resolved**: {len(resolved)} æ¡")
        lines.append(f"- **Failed**: {len(failed)} æ¡")
        lines.append("")
        
        lines.append("#### ğŸ“Š å…¨éƒ¨æ ·æœ¬ç»Ÿè®¡")
        lines.append("")
        lines.append("| æŒ‡æ ‡ | æœ€å¤§å€¼ | æœ€å°å€¼ | **å‡å€¼** | æ–¹å·® | æ ‡å‡†å·® |")
        lines.append("|------|--------|--------|----------|------|--------|")
        
        for key, display_name in metric_keys:
            values = [r["metrics"][key] for r in results if key in r.get("metrics", {})]
            stats = compute_statistics(values)
            lines.append(f"| {display_name} | {stats['max']:.4f} | {stats['min']:.4f} | **{stats['mean']:.4f}** | {stats['variance']:.4f} | {stats['std']:.4f} |")
        
        lines.append("")
        
        lines.append("#### ğŸ” Resolved vs Failed å¯¹æ¯”")
        lines.append("")
        lines.append("| æŒ‡æ ‡ | Resolved å‡å€¼ | Failed å‡å€¼ | å·®å¼‚ |")
        lines.append("|------|---------------|-------------|------|")
        
        for key, display_name in metric_keys:
            resolved_values = [r["metrics"][key] for r in resolved if key in r.get("metrics", {})]
            failed_values = [r["metrics"][key] for r in failed if key in r.get("metrics", {})]
            
            resolved_mean = mean(resolved_values) if resolved_values else 0.0
            failed_mean = mean(failed_values) if failed_values else 0.0
            diff = resolved_mean - failed_mean
            diff_str = f"+{diff:.4f}" if diff >= 0 else f"{diff:.4f}"
            
            lines.append(f"| {display_name} | {resolved_mean:.4f} | {failed_mean:.4f} | {diff_str} |")
        
        lines.append("")
        lines.append("---")
        lines.append("")
    
    lines.append("## ğŸ† æ¨¡å‹é—´å‡å€¼å¯¹æ¯”æ±‡æ€»")
    lines.append("")
    lines.append("> ä»¥ä¸‹è¡¨æ ¼å±•ç¤ºå„æ¨¡å‹åœ¨æ‰€æœ‰æ ·æœ¬ä¸Šçš„**å‡å€¼**å¯¹æ¯”ï¼Œä¾¿äºæ¨ªå‘æ¯”è¾ƒã€‚")
    lines.append("")
    
    model_names = list(all_results.keys())
    header = "| æŒ‡æ ‡ |"
    separator = "|------|"
    for model in model_names:
        header += f" **{model}** |"
        separator += "--------|"
    
    lines.append(header)
    lines.append(separator)
    
    for key, display_name in metric_keys:
        row = f"| {display_name} |"
        values_per_model = []
        
        for model in model_names:
            results = all_results[model]
            values = [r["metrics"][key] for r in results if key in r.get("metrics", {})]
            avg = mean(values) if values else 0.0
            values_per_model.append(avg)
        
        max_val = max(values_per_model) if values_per_model else 0.0
        
        for avg in values_per_model:
            if avg == max_val and max_val > 0:
                row += f" **{avg:.4f}** ğŸ¥‡ |"
            else:
                row += f" {avg:.4f} |"
        
        lines.append(row)
    
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("## ğŸ“ å¤‡æ³¨")
    lines.append("")
    lines.append("- ğŸ¥‡ è¡¨ç¤ºè¯¥æŒ‡æ ‡åœ¨å„æ¨¡å‹ä¸­çš„**æœ€é«˜å€¼**")
    lines.append("- BERTScore ä½¿ç”¨ `bert-base-uncased` æ¨¡å‹")
    lines.append("- BLEU-4 ä½¿ç”¨ `sacrebleu` å®ç°ï¼ˆèŒƒå›´ 0~100ï¼‰")
    lines.append("- ESS/RAS ä½¿ç”¨ spaCy è¿›è¡Œä¾å­˜å¥æ³•åˆ†æ")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("*æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ*")
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    
    print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def main():
    print("=" * 60)
    print("ğŸš€ è¯„æµ‹æŒ‡æ ‡è®¡ç®—ï¼ˆé«˜é€Ÿç‰ˆ v2 - å¤šæ¨¡å‹å¹¶å‘ï¼‰")
    print(f"   CPU æ ¸å¿ƒæ•°: {NUM_TOTAL_CORES}")
    print(f"   å¹¶å‘æ¨¡å‹æ•°: {NUM_MODEL_WORKERS}")
    print(f"   æ¯æ¨¡å‹ ESS/RAS è¿›ç¨‹æ•°: {NUM_ESS_RAS_WORKERS_PER_MODEL}")
    print("=" * 60)
    
    # æ£€æŸ¥æ–­ç‚¹
    completed_models, all_output_items = load_checkpoint()
    if completed_models:
        if os.path.exists(CHECKPOINT_FILE):
            print(f"ğŸ“Œ å‘ç°æ£€æŸ¥ç‚¹ï¼Œå·²å®Œæˆæ¨¡å‹: {completed_models}")
        else:
            print(f"ğŸ“Œ å‘ç°å·²æœ‰è¾“å‡ºï¼Œå·²å®Œæˆæ¨¡å‹: {completed_models}")
        print(f"   å·²æœ‰ç»“æœ: {len(all_output_items)} æ¡")
    elif all_output_items:
        print(f"ğŸ“Œ å‘ç°å·²æœ‰è¾“å‡ºï¼Œä½†æœªè¯†åˆ«åˆ°å·²å®Œæˆæ¨¡å‹ï¼Œå·²æœ‰ç»“æœ: {len(all_output_items)} æ¡")
    
    # è¯»å–å‚è€ƒæ•°æ®
    print(f"\nè¯»å–å‚è€ƒæ•°æ®: {REFERENCE_FILE}")
    reference_data = read_jsonl(REFERENCE_FILE)
    ref_dict = {item["instance_id"]: item for item in reference_data}
    print(f"  å‚è€ƒæ•°æ®æ¡æ•°: {len(ref_dict)}")
    
    all_results = {}
    
    # æ¢å¤å·²å®Œæˆçš„ç»“æœ
    for item in all_output_items:
        # å…¼å®¹æ—§ç‰ˆæœ¬ï¼ˆä½¿ç”¨ modeï¼‰
        model = item.get("model", item.get("mode"))
        if model not in all_results:
            all_results[model] = []
        all_results[model].append(item)

    # ç­›é€‰å¾…å¤„ç†çš„æ¨¡å‹
    pending_models = []
    for model_name, model_file in MODEL_FILES.items():
        if model_name in completed_models:
            print(f"\nâ­ è·³è¿‡å·²å®Œæˆæ¨¡å‹: {model_name}")
            continue
        if not os.path.exists(model_file):
            print(f"\nâš  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {model_file}")
            continue
        pending_models.append((model_name, model_file))
    
    if not pending_models:
        print("\næ‰€æœ‰æ¨¡å‹å·²å®Œæˆï¼")
    else:
        print(f"\nğŸ”„ å¾…å¤„ç†æ¨¡å‹æ•°: {len(pending_models)}")
    
        # ========== å¤šæ¨¡å‹å¹¶å‘å¤„ç† ==========
        with ThreadPoolExecutor(max_workers=NUM_MODEL_WORKERS) as executor:
            futures = {
                executor.submit(
                    process_model_standalone, 
                    model_name, 
                    model_file, 
                    ref_dict, 
                    NUM_ESS_RAS_WORKERS_PER_MODEL
                ): model_name
                for model_name, model_file in pending_models
            }
            
            for future in as_completed(futures):
                model_name, model_results = future.result()
                all_results[model_name] = model_results
                all_output_items.extend(model_results)
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                completed_models.append(model_name)
                save_checkpoint(completed_models, all_output_items)
                safe_print(f"ğŸ’¾ [{model_name}] æ£€æŸ¥ç‚¹å·²ä¿å­˜")
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nç”Ÿæˆåˆ†ææŠ¥å‘Š: {OUTPUT_REPORT}")
    generate_markdown_report(all_results, OUTPUT_REPORT)
    
    # æ¸…ç†æ£€æŸ¥ç‚¹
    if os.path.exists(CHECKPOINT_FILE):
        os.remove(CHECKPOINT_FILE)
    
    print("\n" + "=" * 60)
    print("âœ… è¯„æµ‹å®Œæˆï¼")
    print(f"   è¯¦ç»†ç»“æœ: {OUTPUT_JSONL}")
    print(f"   åˆ†ææŠ¥å‘Š: {OUTPUT_REPORT}")
    print("=" * 60)


if __name__ == "__main__":
    main()
